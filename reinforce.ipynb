{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "29a35977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/python@3.10/bin/python3.10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "397f4569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9493c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b01f09c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_obs(obs):\n",
    "    return obs.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c764c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size=300, hidden_size=128, output_size=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Softmax(dim=-1)   # output is probability distribution\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d4637dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    def __init__(self, lr=0.001, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.policy = PolicyNetwork()\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        self.log_probs = []\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        # forward pass → probability distribution over 100 actions\n",
    "        probs = self.policy(state)\n",
    "\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        self.log_probs.append(dist.log_prob(action))\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def compute_returns(self):\n",
    "        # computes discounted returns for episode\n",
    "        returns = []\n",
    "        R = 0\n",
    "\n",
    "        # backwards through the rewards\n",
    "        for r in reversed(self.episode_rewards):\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        returns = torch.tensor(returns,dtype=torch.float32)\n",
    "\n",
    "        returns = (returns-returns.mean()) /(returns.std()+1e-9)\n",
    "        return returns\n",
    "\n",
    "    def update_policy(self):\n",
    "        # uses reinforce rule\n",
    "        returns = self.compute_returns()\n",
    "\n",
    "        loss = 0\n",
    "        for log_prob, G in zip(self.log_probs, returns):\n",
    "            # gradient ascent → maximize expected reward\n",
    "            loss += -log_prob*G  \n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.log_probs =[]\n",
    "        self.episode_rewards =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "235be822",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_dict = {\n",
    "    'win': 100,\n",
    "    'missed': 0,\n",
    "    'hit': 5,\n",
    "    'proximal_hit': 20,\n",
    "    'repeat_missed': -1,\n",
    "    'repeat_hit': -1,\n",
    "    'sunk_ship_bonus': 5.0,\n",
    "\n",
    "    # same as 'hit'\n",
    "    'touched': 5,         \n",
    "    # same as 'repeat_hit'  \n",
    "    'repeat_touched': -1     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9697e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Battleship-v0\", reward_dictionary=reward_dict)\n",
    "agent = REINFORCEAgent(lr=1e-3, gamma=0.99)\n",
    "\n",
    "num_episodes = 300  \n",
    "episode_rewards = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d726f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs shape: (3, 10, 10)\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "print(\"Obs shape:\", obs.shape)\n",
    "print(obs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
